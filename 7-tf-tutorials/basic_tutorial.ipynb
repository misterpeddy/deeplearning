{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chollet-2.0-Tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "WyO3fpogEAoB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_78nlV6BD9I9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# An example of simple Layer class, without best practices\n",
        "\n",
        "class Linear(tf.keras.layers.Layer):\n",
        "  \"\"\"Implements y = w.x + b\"\"\"\n",
        "  \n",
        "  def __init__(self, units=32, input_dim=32):\n",
        "    super(Linear, self).__init__()\n",
        "    w_init = tf.random_normal_initializer()\n",
        "    self.w = tf.Variable(\n",
        "        initial_value = w_init(shape=(input_dim, units), dtype='float32'),\n",
        "        trainable=True)\n",
        "    b_init = tf.zeros_initializer()\n",
        "    self.b = tf.Variable(\n",
        "        initial_value = b_init(shape=(units, ), dtype='float32'),\n",
        "        trainable=True)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "# Instantiate a Linear Layer\n",
        "linear_layer = Linear(4, 2)\n",
        "\n",
        "# Call it just like a python function\n",
        "y = linear_layer(tf.ones((10, 2)))\n",
        "assert y.shape == (10, 4)\n",
        "\n",
        "# Weights are tracked under the `weights` property\n",
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3j4XxhoIuxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "afe590d9-80cd-460f-d637-f3a4dc3d0688"
      },
      "cell_type": "code",
      "source": [
        "# Same example with best practices \n",
        "# 1) We use .add_weight() shortcut \n",
        "# 2) We use .build() which lazily gets called first time our layer is called\n",
        "# which allows us to not have to know the the input dims at init time\n",
        "class Linear(tf.keras.layers.Layer):\n",
        "  \"\"\"Implements y = w.x + b\"\"\" \n",
        "  \n",
        "  def __init__(self, units=32):\n",
        "    super(Linear, self).__init__()\n",
        "    self.units = units\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    print(\"**Building layer weigths**\")\n",
        "    self.w = self.add_weight(\n",
        "        shape=(input_shape[-1], self.units),\n",
        "        initializer='random_normal',\n",
        "        trainable=True)\n",
        "    self.b = self.add_weight(\n",
        "        shape=(self.units,),\n",
        "        initializer='random_normal',\n",
        "        trainable=True)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "# Instantiate a Linear Layer\n",
        "linear_layer = Linear(4)\n",
        "# Call the layer (call() will invoke build() on first invocation)\n",
        "y = linear_layer(tf.ones((3, 3)))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**Building layer weigths**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EWvo80foMQ-X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare a dataset\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Instantiate a Linear layer with 10 units\n",
        "linear_layer = Linear(10)\n",
        "\n",
        "# Instantiate a logistic loss function expecting logits before softmax layer\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Instantiate an optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "# Iterate over batches of the dataset\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  \n",
        "  # Open a GradientTape\n",
        "  with tf.GradientTape() as tape:\n",
        "    \n",
        "    # Forward pass through our layer\n",
        "    logits = linear_layer(x)\n",
        "    \n",
        "    # Calculate loss\n",
        "    loss = loss_fn(y, logits)\n",
        "    \n",
        "    # Calculate gradients of loss wrt to trainable weights\n",
        "    grads = tape.gradient(loss, linear_layer.trainable_weights)\n",
        "    \n",
        "  # Apply gradients\n",
        "  optimizer.apply_gradients(zip(grads, linear_layer.trainable_weights))\n",
        "\n",
        "  # Logging\n",
        "  if step % 100 == 0:\n",
        "    print(step, float(loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "arFH65K0PIzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1b95b104-075c-4560-c74c-304549668875"
      },
      "cell_type": "code",
      "source": [
        "# We can reuse this layer to compose more complex objects\n",
        "\n",
        "class MLP(tf.keras.layers.Layer):\n",
        "  \"\"\"Simple stack of linear layers.\"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MLP, self).__init__()\n",
        "    self.layer_1 = Linear(32)\n",
        "    self.layer_2 = Linear(32)\n",
        "    self.layer_3 = Linear(10)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    x = self.layer_1(inputs)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.layer_2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    return self.layer_3(x)\n",
        "  \n",
        "mlp = MLP()\n",
        "\n",
        "# A call to mlp function which builds the network \n",
        "y = mlp(tf.ones((1, 6)))\n",
        "\n",
        "# Weights are recursively tracked\n",
        "assert len(mlp.trainable_weights) == 6"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**Building layer weigths**\n",
            "**Building layer weigths**\n",
            "**Building layer weigths**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f6cDppSGWzZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Layers can create losses on their forward pass, great for regularization\n",
        "\n",
        "class ActivityRegularization(tf.keras.layers.Layer):\n",
        "  \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
        "  \n",
        "  def __init__(self, rate=1e-2):\n",
        "    super(ActivityRegularization, self).__init__()\n",
        "    self.rate = rate\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    # Add regularization loss based on the input\n",
        "    self.add_loss(tf.reduce_sum(inputs))\n",
        "    return inputs\n",
        "\n",
        "class SparseMLP(tf.keras.layers.Layer):\n",
        "  \"\"\"Simple stack of linear layers with regularization\"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(SparseMLP, self).__init__()\n",
        "    self.linear_1 = Linear(32)\n",
        "    self.regularization = ActivityRegularization()\n",
        "    self.linear_2 = Linear(10)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.linear_1(inputs)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.regularization(x)\n",
        "    return self.linear_2(x)\n",
        "\n",
        "mlp = SparseMLP()\n",
        "y = mlp(tf.ones((10, 10)))\n",
        "\n",
        "# These are the losses of the last forward pass\n",
        "print(mlp.losses)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ui03wl8he00v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# An example of using internal and external (to the network) losses to train\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
        "\n",
        "mlp = SparseMLP()\n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate=1e-3)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    \n",
        "    # Forward pass\n",
        "    logits = mlp(x)\n",
        "    \n",
        "    # External loss added with regualirzation term\n",
        "    loss = loss_fn(y, logits)\n",
        "    loss += sum(mlp.losses)\n",
        "    \n",
        "    # Calculate gradient of weights wrt loss\n",
        "    grads = tape.gradient(loss, mlp.trainable_weights)  \n",
        "    \n",
        "  # Apply gradients\n",
        "  optimizer.apply_gradients(zip(grads, mlp.trainable_weights))\n",
        "  \n",
        "  if step % 100 == 0:\n",
        "    print(step, float(loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-70fpGJ_jB7z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's use tf.function to wrap a python function in a graph\n",
        "# Prepare our layer, loss, and optimizer.\n",
        "mlp = MLP()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "# Create a training step function.\n",
        "\n",
        "@tf.function  # Make it fast.\n",
        "def train_on_batch(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = mlp(x)\n",
        "    loss = loss_fn(y, logits)\n",
        "    gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "  return loss\n",
        "\n",
        "# Prepare a dataset.\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "  \n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  loss = train_on_batch(x, y)\n",
        "  if step % 100 == 0:\n",
        "    print(step, float(loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ng1BWlJVlhBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "999a118d-d581-4fe4-91a0-9c43afbe111f"
      },
      "cell_type": "code",
      "source": [
        "# Let's write Layers with functionality that differs in training vs. inference\n",
        "class Dropout(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self, rate):\n",
        "    super(Dropout, self).__init__()\n",
        "    self.rate = rate\n",
        "  \n",
        "  @tf.function\n",
        "  def call(self, inputs, training=None):\n",
        "    if training:\n",
        "      return tf.nn.dropout(inputs, rate=self.rate)\n",
        "    return inputs\n",
        "\n",
        "class MLPWithDropout(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MLPWithDropout, self).__init__()\n",
        "    self.linear1 = Linear(32)\n",
        "    self.dropout = Dropout(0.5)\n",
        "    self.linear2 = Linear(10)\n",
        "  \n",
        "  def call(self, inputs, training=None):\n",
        "    x = self.linear1(inputs)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    return self.linear2(x)\n",
        "\n",
        "mlp = MLPWithDropout()\n",
        "y_train = mlp(tf.ones((10,10)), training=True)\n",
        "y_test = mlp(tf.ones((10, 10)), training=False)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**Building layer weigths**\n",
            "**Building layer weigths**\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jW9lMA2sv5Wz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "540e899b-c36f-4118-e8c3-7c7cf50e28b0"
      },
      "cell_type": "code",
      "source": [
        "# `Input` object defines shape and dtype of input data, the deeplearning \n",
        "# equivalent of types in good-old programming languages\n",
        "inputs = tf.keras.Input(shape=(16,))\n",
        "\n",
        "# Calling layers on these \"type\" objects returns new types (new shapes/dtypes)\n",
        "x = Linear(32)(inputs)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Linear(10)(x)\n",
        "\n",
        "# A functional model can be defined for any input/outputs and is a layer itself\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Because we gave it info about its inputs, it already has weights set\n",
        "assert len(model.weights) == 4\n",
        "\n",
        "# You can pass down the training arg which gets passed down to Dropout layer\n",
        "y = model(tf.ones((6, 16)), training=True)\n",
        "assert y.shape == (6, 10)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**Building layer weigths**\n",
            "**Building layer weigths**\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}