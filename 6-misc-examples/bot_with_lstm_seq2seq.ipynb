{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bot with LSTM Seq2Seq",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unclepeddy/deeplearning/blob/master/6-misc-examples/bot_with_lstm_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUFclOm3OsID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGWmQY-7Otgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import logging\n",
        "\n",
        "from numpy import argmax, append as np_append, array as np_array, zeros\n",
        "\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyrhSAAkzD0c",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Prepare Conversation Data\n",
        "\n",
        "Download json file containing the 1:1 thread over which you want to train your language model and preprocess it to extract lists of questions and answers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3OuUz08vrYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the data\n",
        "!wget -O conversation.json http://storage.googleapis.com/peddy-ai-dl-data/fb-messages/kate.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfMwulqivrWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_qa_from_conversation(filename, subject):\n",
        "  \"\"\"Extract question and answers from 1-on-1 thread.\n",
        "  Args:\n",
        "    filename: Name of file containing thread json object downloaded from fb\n",
        "    subject: Name of subject for whom we are training the language model\n",
        "  Returns\n",
        "    (questions, answers) tuple of the same length\n",
        "  \"\"\"\n",
        "\n",
        "  with open(filename) as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "  participants = list(map(lambda x: x['name'], data['participants']))\n",
        "  assert len(participants) == 2\n",
        "  assert subject in participants\n",
        "\n",
        "  raw_messages = data['messages']\n",
        "  \n",
        "  # Ensure we start at a message sent not by the subject\n",
        "  start = None \n",
        "  for start in reversed(range(len(raw_messages))):\n",
        "    if (raw_messages[start]['sender_name'] != subject):\n",
        "      break\n",
        "\n",
        "  assert start > 0\n",
        "\n",
        "  # Aggregate and capture consecutive messages by the same sender\n",
        "  sender = raw_messages[start]['sender_name']\n",
        "  text = \"\"\n",
        "  questions = list()\n",
        "  answers = list()\n",
        "  timestamp = raw_messages[start]['timestamp_ms']\n",
        "\n",
        "  for i in range(start, -1, -1):\n",
        "    message = raw_messages[i]\n",
        "\n",
        "    if (message.get('content') == None or len(message['content']) == 0): \n",
        "      logging.warning('message [%s] did not have content - skipping', message)\n",
        "      continue\n",
        "\n",
        "    # Escape non-ASCII characters\n",
        "    message['content'] = message['content'].encode('ascii', errors='ignore').decode()\n",
        "     \n",
        "    if (message['sender_name'] == sender):\n",
        "      text += \" \" + message['content'].replace('\\n', ' ')\n",
        "    else:\n",
        "      if (sender == subject):\n",
        "        answers.append(\"<BEG>{}<END>\".format(text))\n",
        "      else:\n",
        "        questions.append(text)\n",
        "      sender = message['sender_name']\n",
        "      text = message['content'].replace('\\n', ' ')\n",
        "\n",
        "  # Only append the last message if it were the subject's response\n",
        "  if (sender == subject):\n",
        "    answers.append(\"<BEG>{}<END>\".format(text))\n",
        "  assert len(questions) == len(answers), \"questions contains %d elements while answers contains %d elements.\" % (len(questions), len(answers))\n",
        "\n",
        "  return (questions, answers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjXemSL3vrVf",
        "colab_type": "code",
        "outputId": "46ab86b9-feac-4ad5-dc2a-5ef85aaee55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Build 2 separate lists containing questions and answers\n",
        "(q, a) = extract_qa_from_conversation('conversation.json', 'Pedram Pejman')\n",
        "\n",
        "print(\"Question: {}\\nAnswer: {}\".format(q[0], a[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  Kate sent a photo. rad\n",
            "Answer: <BEG>You can now call each other and see information like Active Status and when you've read messages. When you forget what you were gonna say so you just show off your manicure<END>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8CwZT0vz0g5",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Train a simple Seq2Seq Model\n",
        "\n",
        "We'll use a simple seq2seq architecture to learn a language model over the conversation data we have prepared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi7VTz9s0w98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_texts(texts, tokenizer, one_hot=False):\n",
        "  \"\"\"Returns np.array with texts tokenized and padded to length of longest text.\"\"\"\n",
        "  # TODO(peddy): Add OOV Token handling\n",
        "  tokenized_texts = tokenizer.texts_to_sequences(texts)\n",
        "  # TODO(peddy): extract max_len as a hparam\n",
        "  max_len = max([len(x) for x in tokenized_texts])\n",
        "  padded_texts = pad_sequences(tokenized_texts, maxlen=max_len, padding='post')\n",
        "  if one_hot:\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    padded_texts = to_categorical(padded_texts, vocab_size)\n",
        "  return np_array(padded_texts)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(q + a)\n",
        "tokenizer.reverse_word_index = dict([(v, k) for (k, v) in tokenizer.word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDyA087B8xVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = preprocess_texts(q, tokenizer, False)\n",
        "decoder_input_data = preprocess_texts(a, tokenizer, False)\n",
        "# TODO(peddy): Drop the <BEG> tag from the response\n",
        "decoder_output_data = preprocess_texts(a, tokenizer, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ5SGXli9PD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 200\n",
        "lstm_units = 200\n",
        "epochs = 4\n",
        "batch_size = 4\n",
        "max_len = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL3gvSrL6Doc",
        "colab_type": "code",
        "outputId": "50e6f24a-8cf2-4732-84f4-f447ebc98363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "enc_inputs = Input(shape=(None, ), name='enc_inputs')\n",
        "enc_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True, name='enc_embedding')(enc_inputs)\n",
        "_, enc_state_h, enc_state_c = LSTM(lstm_units, return_state=True, name='enc_lstm')(enc_embedding)\n",
        "enc_states = [enc_state_h, enc_state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None, ), name='dec_inputs')\n",
        "dec_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True, name='dec_embedding')(dec_inputs)\n",
        "dec_lstm = LSTM(lstm_units, return_state=True, return_sequences=True)\n",
        "dec_lstm_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_softmax = Dense(vocab_size, activation=softmax, name='dec_softmax')\n",
        "dec_outputs = dec_softmax(dec_lstm_outputs)\n",
        "\n",
        "training_model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
        "training_model.compile(loss=categorical_crossentropy, optimizer=RMSprop())\n",
        "\n",
        "training_model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "enc_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "enc_embedding (Embedding)       (None, None, 200)    85000       enc_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dec_embedding (Embedding)       (None, None, 200)    85000       dec_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "enc_lstm (LSTM)                 [(None, 200), (None, 320800      enc_embedding[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, None, 200),  320800      dec_embedding[0][0]              \n",
            "                                                                 enc_lstm[0][1]                   \n",
            "                                                                 enc_lstm[0][2]                   \n",
            "__________________________________________________________________________________________________\n",
            "dec_softmax (Dense)             (None, None, 425)    85425       lstm[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 897,025\n",
            "Trainable params: 897,025\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK3fYG9XXA9t",
        "colab_type": "code",
        "outputId": "11c90816-e86b-47b9-ea23-f41740ee9c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "training_model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=batch_size, epochs=epochs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 23 samples\n",
            "Epoch 1/4\n",
            "23/23 [==============================] - 11s 482ms/sample - loss: 1.3323\n",
            "Epoch 2/4\n",
            "23/23 [==============================] - 2s 86ms/sample - loss: 1.1791\n",
            "Epoch 3/4\n",
            "23/23 [==============================] - 2s 86ms/sample - loss: 1.1197\n",
            "Epoch 4/4\n",
            "23/23 [==============================] - 2s 86ms/sample - loss: 1.0902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f60f11a9e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmkm1w-mZlEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build inference model\n",
        "\n",
        "inf_dec_state_h_input = Input(shape=(lstm_units, ), name='inf_dec_state_h_input')\n",
        "inf_dec_state_c_input = Input(shape=(lstm_units, ), name='inf_dec_state_c_input')\n",
        "inf_dec_state_inputs = [inf_dec_state_h_input, inf_dec_state_c_input]\n",
        "\n",
        "inf_dec_lstm_outputs, inf_dec_state_h, inf_dec_state_c = dec_lstm(dec_embedding, initial_state=inf_dec_state_inputs)\n",
        "inf_dec_state = [inf_dec_state_h, inf_dec_state_c]\n",
        "inf_dec_outputs = dec_softmax(inf_dec_lstm_outputs)\n",
        "\n",
        "inf_encoder = Model(enc_inputs, enc_states)\n",
        "inf_decoder = Model([dec_inputs] + inf_dec_state_inputs, [inf_dec_outputs] + inf_dec_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJeCzZSDp9Nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.reverse_word_index[0] = \"OOV\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wcB4mmHd6CM",
        "colab_type": "code",
        "outputId": "9b80b1de-ba19-465d-e024-f4883edf56ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_sentence = \"do you have the whole thing\"\n",
        "\n",
        "tokens = preprocess_texts([example_sentence], tokenizer)\n",
        "\n",
        "dec_states = inf_encoder.predict(tokens)\n",
        "dec_seq = zeros((1, 1))\n",
        "terminate = False\n",
        "pred_tokens = list()\n",
        "\n",
        "while not terminate:\n",
        "  dec_pred, h, c = inf_decoder.predict([dec_seq] + dec_states)\n",
        "  pred_word_index = argmax(dec_pred[0, -1, :])\n",
        "  pred_word = tokenizer.reverse_word_index[pred_word_index]\n",
        "  pred_tokens.append(pred_word)\n",
        "  dec_seq = zeros((1,1))\n",
        "  dec_seq[0, 0] = pred_word_index \n",
        "  dec_states = [h, c]\n",
        "  if (pred_word == 'end' or len(pred_tokens) > max_len):\n",
        "    break\n",
        "print(pred_tokens)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['beg', 'beg', 'beg', 'you', 'you', 'you', 'you', 'you', 'you', 'end']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}